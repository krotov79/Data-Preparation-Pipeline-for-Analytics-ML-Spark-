# Data Preparation Pipeline for Analytics & ML (Spark)

End-to-end ETL workflow built with **PySpark** to process and analyze New York City Yellow Taxi trip data.  
The pipeline reads public **Parquet** data, performs cleaning and feature extraction, joins external lookup tables (pickup/drop-off zones), computes daily revenue and tip aggregations, runs basic data-quality checks, and produces a visualization of daily earnings.

---

### ğŸ§° Stack
**PySpark Â· Pandas Â· Matplotlib Â· Parquet**

### âš™ï¸ Features
- Modular ETL scripts for ingestion, transformation, and enrichment  
- Partitioned Parquet outputs for efficient querying  
- Automated workflow via `Makefile` (`ETL â†’ Aggregation â†’ Zone Join â†’ DQ â†’ Plot`)  
- Lightweight data-quality validator (`scripts/dq_checks.py`)  
- Reproducible single-command run: `make all`

### ğŸ¯ Outcome
A clean, reproducible Spark pipeline demonstrating practical big-data handling,  
data-quality validation, and simple analytics â€” easily extendable to cloud-scale environments (e.g., Delta Lake or multi-month batches).

---

## ğŸ§¾ Data Quality & Validation

A small Spark-based checker validates pipeline outputs:

```bash
python scripts/dq_checks.py

